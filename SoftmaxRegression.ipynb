{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9796249,"sourceType":"datasetVersion","datasetId":6003455},{"sourceId":1536558,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Softmax* ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:41:38.720133Z","iopub.execute_input":"2024-11-07T16:41:38.720567Z","iopub.status.idle":"2024-11-07T16:41:38.725868Z","shell.execute_reply.started":"2024-11-07T16:41:38.720512Z","shell.execute_reply":"2024-11-07T16:41:38.724623Z"}},"outputs":[],"execution_count":240},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/softmaxdata/Iris.csv')\ndata['Species'] = data['Species'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica':2})\ndata.columns = ['ID', 'X0', 'X1', 'X2', 'X3', 'Y']\nX = np.array([data['X0'], data['X1'], data['X2'], data['X3']])\nX = X.T\ny = np.array(data['Y'])\ndata.sample(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:41:38.728684Z","iopub.execute_input":"2024-11-07T16:41:38.729824Z","iopub.status.idle":"2024-11-07T16:41:38.758594Z","shell.execute_reply.started":"2024-11-07T16:41:38.729767Z","shell.execute_reply":"2024-11-07T16:41:38.757304Z"}},"outputs":[{"execution_count":241,"output_type":"execute_result","data":{"text/plain":"      ID   X0   X1   X2   X3  Y\n53    54  5.5  2.3  4.0  1.3  1\n39    40  5.1  3.4  1.5  0.2  0\n73    74  6.1  2.8  4.7  1.2  1\n135  136  7.7  3.0  6.1  2.3  2\n111  112  6.4  2.7  5.3  1.9  2\n16    17  5.4  3.9  1.3  0.4  0\n58    59  6.6  2.9  4.6  1.3  1\n54    55  6.5  2.8  4.6  1.5  1\n99   100  5.7  2.8  4.1  1.3  1\n126  127  6.2  2.8  4.8  1.8  2\n67    68  5.8  2.7  4.1  1.0  1\n49    50  5.0  3.3  1.4  0.2  0\n70    71  5.9  3.2  4.8  1.8  1\n68    69  6.2  2.2  4.5  1.5  1\n122  123  7.7  2.8  6.7  2.0  2\n108  109  6.7  2.5  5.8  1.8  2\n28    29  5.2  3.4  1.4  0.2  0\n10    11  5.4  3.7  1.5  0.2  0\n107  108  7.3  2.9  6.3  1.8  2\n74    75  6.4  2.9  4.3  1.3  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>X0</th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>53</th>\n      <td>54</td>\n      <td>5.5</td>\n      <td>2.3</td>\n      <td>4.0</td>\n      <td>1.3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>5.1</td>\n      <td>3.4</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>74</td>\n      <td>6.1</td>\n      <td>2.8</td>\n      <td>4.7</td>\n      <td>1.2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>136</td>\n      <td>7.7</td>\n      <td>3.0</td>\n      <td>6.1</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>112</td>\n      <td>6.4</td>\n      <td>2.7</td>\n      <td>5.3</td>\n      <td>1.9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.3</td>\n      <td>0.4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>59</td>\n      <td>6.6</td>\n      <td>2.9</td>\n      <td>4.6</td>\n      <td>1.3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>55</td>\n      <td>6.5</td>\n      <td>2.8</td>\n      <td>4.6</td>\n      <td>1.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>100</td>\n      <td>5.7</td>\n      <td>2.8</td>\n      <td>4.1</td>\n      <td>1.3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>127</td>\n      <td>6.2</td>\n      <td>2.8</td>\n      <td>4.8</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>68</td>\n      <td>5.8</td>\n      <td>2.7</td>\n      <td>4.1</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>50</td>\n      <td>5.0</td>\n      <td>3.3</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>71</td>\n      <td>5.9</td>\n      <td>3.2</td>\n      <td>4.8</td>\n      <td>1.8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>69</td>\n      <td>6.2</td>\n      <td>2.2</td>\n      <td>4.5</td>\n      <td>1.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>123</td>\n      <td>7.7</td>\n      <td>2.8</td>\n      <td>6.7</td>\n      <td>2.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>109</td>\n      <td>6.7</td>\n      <td>2.5</td>\n      <td>5.8</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>5.2</td>\n      <td>3.4</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>5.4</td>\n      <td>3.7</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>108</td>\n      <td>7.3</td>\n      <td>2.9</td>\n      <td>6.3</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>75</td>\n      <td>6.4</td>\n      <td>2.9</td>\n      <td>4.3</td>\n      <td>1.3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":241},{"cell_type":"code","source":"numClass = 3 # tức là y chỉ có các giá trị 0, 1, 2 \ny = oneHotEncoding(y, numClass) # cho giá trị 0 là 1000, 1 là 0100, 2 là 0010\nw = np.random.randn(X.shape[1], numClass) # tạo ra ma trận w với các giá trị ngẫu nhiên 4 * 3\nlearningRate = 0.5\nepochs = 1000\n\ndef oneHotEncoding(y, numClass):\n    m = y.shape[0] # m = 150\n    yOneHot = np.zeros((m, numClass)) # ma trận 150 * 3 toàn số 0\n    yOneHot[np.arange(m), y] = 1 # m là ma trận từ 0 -> 149 và y là ma trận 150 số từ 0 -> 2\n    return yOneHot\ndef crossEntropyLoss(y, yPre):\n    m = y.shape[0]\n    logLikelihood = -np.sum(y * np.log(yPre))\n    loss = np.sum(logLikelihood) / m\n    return loss\ndef softmax(z):\n    ez = np.exp(z - np.max(z))\n    return ez/ez.sum(axis=1, keepdims=True)\ndef gradientDescent(X, y, w, learningRate, epochs):\n    m = X.shape[0]\n    for it in range(epochs):\n        z = np.dot(X, w)\n        yPre = softmax(z)\n        loss = crossEntropyLoss(y, yPre)\n        dW = np.dot(X.T, (yPre - y)) / m\n        w -= learningRate * dW\n        if it % 100 == 0:\n            print(f'Loss: {loss}')\n\n    return w\nfinalW = gradientDescent(X, y, w, learningRate, epochs) # tìm ra w cuối cùng\nfinalW\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:41:38.760196Z","iopub.execute_input":"2024-11-07T16:41:38.760675Z","iopub.status.idle":"2024-11-07T16:41:38.836567Z","shell.execute_reply.started":"2024-11-07T16:41:38.760621Z","shell.execute_reply":"2024-11-07T16:41:38.835487Z"}},"outputs":[{"name":"stdout","text":"Loss: 3.491264033546976\nLoss: 0.488480164692876\nLoss: 1.4700767752084518\nLoss: 0.07954174328160128\nLoss: 0.07779186184407841\nLoss: 0.07654110067566558\nLoss: 0.07563638178122574\nLoss: 0.07497591814051564\nLoss: 0.07448896977690286\nLoss: 0.0741257436563466\n","output_type":"stream"},{"execution_count":242,"output_type":"execute_result","data":{"text/plain":"array([[ 3.14090222,  2.6083308 , -4.26787399],\n       [ 5.17181616,  1.7978828 , -4.31114692],\n       [-6.6071681 , -2.00670818,  6.95945996],\n       [-2.79564128, -2.02698699,  7.8465281 ]])"},"metadata":{}}],"execution_count":242},{"cell_type":"code","source":"\n\n# def oneHotEncoding(y, numClasses):\n#     m = y.shape[0]\n#     Y = np.zeros((m, numClasses))\n#     Y[np.arange(m), y] = 1\n#     return Y\n    \n# def softmax(z):\n#     eZ = np.exp(z)\n#     return eZ/eZ.sum(axis=1, keepdims=True)\n    \n# def crossEntropyLoss(yTrue, yPre):\n#     m = yTrue.shape[0]\n#     logLikelihood = -np.sum(yTrue * np.log(yPre))\n#     loss = np.sum(logLikelihood)/m\n#     return loss\n    \n# def gradientDescent(X, y, w, learningRate=0.5, epochs=1000):\n#     m = X.shape[0]\n#     for it in range(epochs):\n#         z = np.dot(X, w)\n#         yPre = softmax(z)\n#         loss = crossEntropyLoss(y, yPre)\n#         gradient = np.dot(X.T, (yPre - y)) / m\n#         w -= learningRate * gradient\n#         if it % 100 == 0:\n#             print(f'Loss: {loss}')\n#     return w\n    \n# numClasses = 3\n# y = oneHotEncoding(y, 3)\n# w = np.random.randn(X.shape[1], numClasses)\n# finalW = gradientDescent(X, y, w, learningRate=0.5, epochs=1000)\n# print(\"Trọng số sau khi huấn luyện:\")\n# print(finalW)\n\n\n\n\n\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:41:38.837882Z","iopub.execute_input":"2024-11-07T16:41:38.838216Z","iopub.status.idle":"2024-11-07T16:41:38.844075Z","shell.execute_reply.started":"2024-11-07T16:41:38.838179Z","shell.execute_reply":"2024-11-07T16:41:38.842971Z"}},"outputs":[],"execution_count":243}]}